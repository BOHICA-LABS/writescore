================================================================================
                        WRITESCORE - DIMENSION INVENTORY
                         Version 6.0.0 - 16 Dimensions
================================================================================

TIER DISTRIBUTION BY WEIGHT:
┌──────────────────────────────────────────────────────────────────┐
│                                                                  │
│ ADVANCED TIER (42.0%)    CORE TIER (38.0%)    SUPPORTING (20.0%) │
│ ─────────────────────    ───────────────────   ──────────────── │
│ • Predictability 20%     • Readability 10%     • Sentiment 17%   │
│ • Adv Lexical 14%        • Burstiness 6%       • Lexical 3%      │
│ • Pragmatic 4%           • Transition 6%                          │
│ • Perplexity 2%          • Voice 5%                               │
│ • Syntactic 2%           • Formatting 4%                          │
│                          • Structure 4%                           │
│                          • AI Vocabulary 3%                       │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘

DETAILED BREAKDOWN:

1. PREDICTABILITY (20.0%) ■■■■■■■■■■■■■■■■■■■■ [ADVANCED - GLTR, 95% accuracy]
   Features: Token rank distribution, top-10/100/1000 percentages
   Performance: 2-10s first run, 0.1-0.5s cached | Timeout: 120s

2. SENTIMENT (17.0%)     ■■■■■■■■■■■■■■■■■    [SUPPORTING - DistilBERT]
   Features: Emotional variation, flatness detection, mood tracking
   Performance: 50-100ms per chunk (lazy-loaded)

3. ADV LEXICAL (14.0%)   ■■■■■■■■■■■■■■       [ADVANCED - scipy.hypergeom]
   Features: HDD, Yule's K, MATTR, RTTR, Maas diversity metrics
   Performance: O(vocab_size), optimized via textacy/spacy

4. PERPLEXITY (2.0%)     ■■                   [ADVANCED - GPT-2 language model]
   Features: Mathematical perplexity = exp(-(1/N) × Σ log P(w_i | context))
   Features: 40% discrimination (Human median: 35.9, AI median: 21.2)
   Performance: ~2-3s per 1k words (first run), cached model reuse

5. READABILITY (10.0%)   ■■■■■■■■■■           [CORE - textstat/nltk]
   Features: Flesch, Gunning Fog, ARI, word/sentence length

6. TRANSITION (6.0%)     ■■■■■■               [CORE - Discourse markers]
   Features: Transition markers, formulaic transitions (Furthermore, Moreover...)
   Features: AI marker overuse detection, clustering patterns

7. BURSTINESS (6.0%)     ■■■■■■               [CORE - Length variation]
   Features: Sentence/paragraph variance, uniformity detection

8. VOICE (5.0%)          ■■■■■                [CORE - Personal authenticity]
   Features: First-person, contractions, domain expertise

9. SEMANTIC (5.0%)       ■■■■■                [SUPPORTING - Transformer embeddings]
   Features: Paragraph cohesion, topic consistency, discourse flow, conceptual depth
   Performance: Uses all-MiniLM-L6-v2, fallback to lexical overlap

10. PRAGMATIC (4.0%)     ■■■■                 [ADVANCED - Epistemic markers]
   Features: Hedging (might, may, perhaps), certainty markers, speech acts
   Features: Epistemic verbs (assume, estimate), frequency hedges

11. FORMATTING (4.0%)    ■■■■                 [CORE - Em-dash (95% accuracy)]
    Features: Bold/italic, quotations, mechanical consistency

12. STRUCTURE (4.0%)     ■■■■                 [CORE - Heading/section analysis]
    Features: Depth, parallelism, uniformity, list nesting

13. AI VOCABULARY (3.0%) ■■■                  [CORE - 34 AI-typical terms]
    Features: Tier-weighted patterns (delve, robust, leverage, ecosystem...)
    Features: Tier 1 (14 patterns), Tier 2 (12 patterns), Tier 3 (8 patterns)

14. FIGURATIVE (3.0%)    ■■■                  [SUPPORTING - Metaphors/similes/idioms]
    Features: Diversity/novelty detection, cliché ratio, type variety
    Performance: < 30s per 10k words, 83-90% accuracy, embedding-based

15. LEXICAL (3.0%)       ■■■                  [SUPPORTING - Type-Token Ratio]
    Features: Vocabulary richness, word frequency distribution

16. SYNTACTIC (2.0%)     ■■                   [ADVANCED - spacy dependency trees]
    Features: Tree depth, subordination, passive voice, POS diversity

================================================================================
                           FEATURE COVERAGE SUMMARY
================================================================================

FULLY COVERED (70+ features):
  ✓ Lexical/Vocabulary:     27+ AI terms + 18 transitions + word frequency
  ✓ Syntactic/Grammar:      Dependency trees, subordination, passive voice
  ✓ Readability:            Flesch, Gunning Fog, ARI, syllable patterns
  ✓ Stylistic:              Em-dashes, bold/italic, heading analysis
  ✓ Semantic/Pragmatic:     Sentiment, first-person, contractions
  ✓ ML-Based:               GLTR (95%), DistilBERT sentiment

NOT COVERED (Opportunities for StyloMetrix):
  ✗ Figurative Language:    Metaphor, simile, irony detection
  ✗ Pragmatic Markers:      Speech acts, hedging, certainty levels
  ✗ Author Fingerprinting:  Unique stylistic signatures
  ✗ Multi-Model Ensemble:   GPT-3.5, GPT-4, Claude, Gemini comparison
  ✗ Semantic Coherence:     Cross-sentence continuity, topic flow
  ✗ Information Structure:   Density, argument flow, paragraph roles

================================================================================
                        ARCHITECTURE & INTEGRATION
================================================================================

DESIGN PATTERN: Self-Registering DimensionStrategy
  ┌─────────────────────────────────────────────────────────────┐
  │  New Dimension = ZERO core modifications required!         │
  │  1. Create class implementing DimensionStrategy             │
  │  2. Call DimensionRegistry.register(self)                   │
  │  3. Analyzer auto-discovers and integrates                  │
  │  4. No configuration files or hardcoding needed             │
  └─────────────────────────────────────────────────────────────┘

CONFIGURATION FLEXIBILITY:
  • Profiles: fast (4 dims), balanced (8 dims), full (16 dims)
  • Modes: FAST (2-5s), ADAPTIVE (5-15s), SAMPLING (10-30s), FULL (30-120s)
  • Thread-safe: All global state protected with locks
  • Lazy-loading: ML models loaded only on first use

PERFORMANCE CHARACTERISTICS:
  • Memory: 700-1000MB full / 100-150MB fast profile
  • Caching: GLTR models cached module-level (0.1-0.5s subsequent)
  • Concurrency: Thread-safe for parallel document analysis
  • Bottlenecks: Model loading (mitigated), sentiment inference (optimized)

================================================================================
                           VERSION HISTORY
================================================================================

v6.0.0 (CURRENT - 2025-11-22):
  ✓ Renamed from "AI Pattern Analyzer" to "WriteScore"
  ✓ Expanded from 15 → 16 dimensions
  ✓ Added AI Vocabulary (3% - extracted from perplexity stub)
  ✓ Added Pragmatic Markers (4% - extracted from transition marker)
  ✓ Implemented True Perplexity (2% - GPT-2 mathematical perplexity)
  ✓ Updated Transition Markers (10% → 6%, moved to CORE tier)
  ✓ Refactored dimension structure for clearer separation of concerns
  ✓ All 57 perplexity tests passing with 15x performance optimization

v5.0.0 (Previous - Breaking Changes):
  ✓ Removed deprecated 'advanced' dimension (655 lines)
  ✓ Removed deprecated 'stylometric' dimension (378 lines)
  ✓ Total cleanup: 1,033 lines removed
  ✓ New intuitive labels: EXCELLENT/GOOD/NEEDS WORK/POOR
  ✓ No backward compatibility (designed for clean slate)

v4.x (Historical):
  • 14 dimensions (advanced + stylometric)
  • Confusing inverse labels (HIGH = good, LOW = bad)
  • Backward compatibility shims

================================================================================
                     STYLOMETRX INTEGRATION GUIDANCE
================================================================================

ALREADY COVERED - DON'T DUPLICATE:
  ✓ Readability metrics (Flesch, Gunning Fog, ARI)
  ✓ Lexical diversity (TTR, HDD, Yule's K, MATTR, RTTR, Maas)
  ✓ Sentiment analysis (emotional variation)
  ✓ Formatting patterns (em-dashes, bold/italic)
  ✓ Syntactic features (tree depth, subordination, passive voice)

RECOMMENDED FOR STYLOMETRX:
  1. Figurative Language Detection (metaphor, simile, personification)
  2. Pragmatic Analysis (speech acts, hedging, certainty markers)
  3. Author Fingerprinting (unique stylistic signatures across docs)
  4. Multi-Model Comparison (detect model-specific patterns)
  5. Information Density (entropy, progression patterns)
  6. Cross-Document Analysis (consistency, topic drift)

INTEGRATION REQUIREMENTS:
  • Implement DimensionStrategy base class
  • Self-register with DimensionRegistry
  • Weight: 5-10% (avoid conflicting with predictability's 20%)
  • Performance: Lazy-load ML models, thread-safe caching
  • Optional: analyze_detailed() for CLI findings

================================================================================
                              KEY STATISTICS
================================================================================

Codebase Metrics:
  • Total Production Code: ~7,770 lines (dimensions only)
  • Largest File: structure.py (1,640 lines)
  • Smallest File: syntactic.py (525 lines)
  • Total Features: 70+ stylometric/linguistic
  • Architecture: Self-registering, zero-modification pattern

Dependencies:
  • Required: marko, nltk, spacy, textstat, transformers, torch, scipy, textacy
  • Optional: click (CLI), pytest (testing)
  • Python: 3.8+

Testing:
  • Regression Suite: Full baseline score testing
  • Integration Tests: 12+ dimension interaction tests
  • Unit Tests: Per-dimension feature coverage
  • Performance: Timeout enforcement, caching validation

Documentation:
  • README: Comprehensive with examples
  • CHANGELOG: Detailed v5.0.0 changes
  • MIGRATION: Step-by-step v4.x to v5.0.0 guide
  • Analysis Modes: 500+ lines of mode documentation

================================================================================
