# Story 7.1: WriteScore Calibration Corpus Acquisition

## Status

Proposed

## Story

**As a** WriteScore developer,
**I want** to acquire and organize a comprehensive corpus of verified human-written text from multiple genres and content types,
**so that** I can calibrate dimension thresholds, validate scoring accuracy, and ensure WriteScore performs reliably across diverse writing styles.

## Background

WriteScore's AI detection accuracy depends on well-calibrated thresholds derived from authentic human writing samples. Current calibration relies on limited test fixtures. A rigorous calibration corpus requires:

1. **Verified human authorship**: Pre-2022 content (before ChatGPT) or known human authors
2. **Genre diversity**: Technical, business, academic, journalistic, informal, and review content
3. **Style variety**: Formal to casual, simple to complex, short to long-form
4. **Sufficient volume**: 1000+ documents per category for statistical significance

### Research-Identified Datasets

The following publicly available datasets meet our criteria:

| Category | Dataset | Size | License | Pre-2022 |
|----------|---------|------|---------|----------|
| Business | Enron Email Corpus | 500k emails | Public Domain | Yes (2001) |
| Reviews | Yelp Open Dataset | 7M reviews | Academic | Mostly |
| Academic | ASAP 2.0 Essays | 25k essays | Academic | Yes |
| Technical | Stack Overflow | 23M posts | CC BY-SA 4.0 | Yes |
| Reviews | Amazon Reviews | 233M reviews | Academic | Yes |
| News | CNN/DailyMail | 300k articles | Apache 2.0 | Yes |
| Informal | Reddit Corpus | 1.7B comments | Research | Yes |
| Academic | arXiv Papers | 2.4M papers | Open Access | Yes |
| News | Reuters RCV1 | 800k articles | Research | Yes (1996-97) |
| Academic | ICLE (learner) | 6k essays | Academic | Yes |
| Academic | PubMed | 36M abstracts | Public | Yes |

## Acceptance Criteria

1. **Corpus Structure**: Organized directory structure at `data/calibration/` with subdirectories per genre
2. **Dataset Acquisition**: At least 6 diverse datasets downloaded and processed (minimum 3 categories)
3. **Preprocessing Pipeline**: Scripts to normalize, clean, and standardize text format
4. **Metadata Tracking**: JSON manifest tracking source, license, document count, and pre-2022 verification
5. **Sampling Strategy**: Representative sampling for large datasets (10k-50k docs per corpus)
6. **Quality Filters**: Remove duplicates, spam, extremely short/long documents, and non-English content
7. **Dimension Baselines**: Generate baseline metrics for each dimension across all corpora
8. **Statistical Summary**: Report mean, std, percentiles (5th, 25th, 50th, 75th, 95th) per dimension per genre
9. **Documentation**: README explaining corpus contents, acquisition process, and usage guidelines
10. **Git LFS**: Large files managed with Git LFS or external storage with download scripts

## Tasks / Subtasks

### Phase 1: Infrastructure Setup

- [ ] **Task 1.1**: Create corpus directory structure (AC: 1)
  - [ ] Subtask 1.1.1: Create `data/calibration/` directory
  - [ ] Subtask 1.1.2: Create genre subdirectories: `business/`, `reviews/`, `academic/`, `technical/`, `news/`, `informal/`
  - [ ] Subtask 1.1.3: Create `scripts/corpus/` for acquisition and processing scripts
  - [ ] Subtask 1.1.4: Create manifest template `data/calibration/manifest.json`

- [ ] **Task 1.2**: Set up Git LFS for large files (AC: 10)
  - [ ] Subtask 1.2.1: Configure `.gitattributes` for corpus files
  - [ ] Subtask 1.2.2: Document storage strategy in README
  - [ ] Subtask 1.2.3: Create download scripts for external datasets

### Phase 2: Dataset Acquisition

- [ ] **Task 2.1**: Acquire Enron Email Corpus - Business/Professional (AC: 2, 3)
  - [ ] Subtask 2.1.1: Download from CMU (https://www.cs.cmu.edu/~enron/)
  - [ ] Subtask 2.1.2: Parse maildir format to plain text
  - [ ] Subtask 2.1.3: Extract email body, remove headers/signatures
  - [ ] Subtask 2.1.4: Sample 25k emails across senders
  - [ ] Subtask 2.1.5: Store in `data/calibration/business/enron/`

- [ ] **Task 2.2**: Acquire Yelp Open Dataset - Reviews (AC: 2, 3)
  - [ ] Subtask 2.2.1: Download from Yelp Dataset Challenge
  - [ ] Subtask 2.2.2: Parse JSON review data
  - [ ] Subtask 2.2.3: Filter pre-2022 reviews only
  - [ ] Subtask 2.2.4: Sample 50k reviews (varied star ratings and lengths)
  - [ ] Subtask 2.2.5: Store in `data/calibration/reviews/yelp/`

- [ ] **Task 2.3**: Acquire ASAP 2.0 Essays - Academic (AC: 2, 3)
  - [ ] Subtask 2.3.1: Download from Kaggle/original source
  - [ ] Subtask 2.3.2: Parse essay files
  - [ ] Subtask 2.3.3: Include all 25k essays (already curated)
  - [ ] Subtask 2.3.4: Store in `data/calibration/academic/asap/`

- [ ] **Task 2.4**: Acquire Stack Overflow Data - Technical (AC: 2, 3)
  - [ ] Subtask 2.4.1: Download from Stack Exchange Data Dump
  - [ ] Subtask 2.4.2: Extract question bodies and accepted answers
  - [ ] Subtask 2.4.3: Filter by score (>5) for quality
  - [ ] Subtask 2.4.4: Filter pre-2022 content
  - [ ] Subtask 2.4.5: Sample 50k posts across tags
  - [ ] Subtask 2.4.6: Store in `data/calibration/technical/stackoverflow/`

- [ ] **Task 2.5**: Acquire Amazon Reviews - Reviews (AC: 2, 3)
  - [ ] Subtask 2.5.1: Download from Amazon Review Data (McAuley lab)
  - [ ] Subtask 2.5.2: Parse JSON review data
  - [ ] Subtask 2.5.3: Sample across categories (Books, Electronics, etc.)
  - [ ] Subtask 2.5.4: Sample 50k reviews (varied ratings and lengths)
  - [ ] Subtask 2.5.5: Store in `data/calibration/reviews/amazon/`

- [ ] **Task 2.6**: Acquire CNN/DailyMail - News/Journalism (AC: 2, 3)
  - [ ] Subtask 2.6.1: Download from Hugging Face or original source
  - [ ] Subtask 2.6.2: Extract article texts
  - [ ] Subtask 2.6.3: Sample 25k articles
  - [ ] Subtask 2.6.4: Store in `data/calibration/news/cnn_dailymail/`

### Phase 3: Preprocessing Pipeline

- [ ] **Task 3.1**: Create preprocessing module (AC: 3, 6)
  - [ ] Subtask 3.1.1: Create `scripts/corpus/preprocess.py`
  - [ ] Subtask 3.1.2: Implement text normalization (Unicode, whitespace, encoding)
  - [ ] Subtask 3.1.3: Implement deduplication (exact and near-duplicate detection)
  - [ ] Subtask 3.1.4: Implement language detection (filter non-English)
  - [ ] Subtask 3.1.5: Implement length filtering (min 100, max 50000 chars)
  - [ ] Subtask 3.1.6: Implement spam/boilerplate detection

- [ ] **Task 3.2**: Create acquisition orchestration (AC: 2, 4)
  - [ ] Subtask 3.2.1: Create `scripts/corpus/acquire.py` CLI tool
  - [ ] Subtask 3.2.2: Implement dataset-specific downloaders
  - [ ] Subtask 3.2.3: Implement progress tracking and resumption
  - [ ] Subtask 3.2.4: Update manifest.json after each acquisition
  - [ ] Subtask 3.2.5: Generate acquisition report

### Phase 4: Baseline Generation

- [ ] **Task 4.1**: Create baseline analysis script (AC: 7, 8)
  - [ ] Subtask 4.1.1: Create `scripts/corpus/generate_baselines.py`
  - [ ] Subtask 4.1.2: Run each dimension analyzer on corpus samples
  - [ ] Subtask 4.1.3: Calculate per-dimension statistics (mean, std, percentiles)
  - [ ] Subtask 4.1.4: Calculate per-genre breakdowns
  - [ ] Subtask 4.1.5: Generate baseline report JSON

- [ ] **Task 4.2**: Statistical analysis and visualization (AC: 8)
  - [ ] Subtask 4.2.1: Create statistical summary per dimension
  - [ ] Subtask 4.2.2: Identify optimal threshold candidates
  - [ ] Subtask 4.2.3: Document findings for dimension calibration stories

### Phase 5: Documentation

- [ ] **Task 5.1**: Create corpus documentation (AC: 9)
  - [ ] Subtask 5.1.1: Create `data/calibration/README.md`
  - [ ] Subtask 5.1.2: Document each dataset's source, license, and characteristics
  - [ ] Subtask 5.1.3: Document preprocessing steps
  - [ ] Subtask 5.1.4: Document usage guidelines for dimension calibration
  - [ ] Subtask 5.1.5: Add citations for all datasets

## Dev Notes

### Dataset Details

#### Enron Email Corpus
- **Source**: https://www.cs.cmu.edu/~enron/
- **Format**: Maildir (nested directories with .txt files)
- **License**: Public Domain (released in FERC investigation)
- **Size**: ~500k emails from 150 users
- **Pre-2022**: Yes (2000-2001)
- **Characteristics**: Professional business email, varied formality, real-world communication

#### Yelp Open Dataset
- **Source**: https://www.yelp.com/dataset
- **Format**: JSON lines
- **License**: Academic/Non-commercial
- **Size**: 7M+ reviews
- **Pre-2022**: Filter by date
- **Characteristics**: Consumer reviews, varied length and quality, authentic opinions

#### ASAP 2.0 (Automated Student Assessment Prize)
- **Source**: Kaggle / https://www.kaggle.com/c/asap-aes
- **Format**: TSV with essays and scores
- **License**: Academic research
- **Size**: ~25k essays across 8 prompts
- **Pre-2022**: Yes (2012)
- **Characteristics**: Student essays, graded, varied quality levels

#### Stack Overflow Data Dump
- **Source**: https://archive.org/details/stackexchange
- **Format**: XML (Posts.xml)
- **License**: CC BY-SA 4.0
- **Size**: 23M+ questions/answers
- **Pre-2022**: Filter by creation date
- **Characteristics**: Technical Q&A, code mixed with prose, expertise levels vary

#### Amazon Reviews
- **Source**: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/
- **Format**: JSON lines
- **License**: Academic research
- **Size**: 233M reviews across categories
- **Pre-2022**: Yes
- **Characteristics**: Product reviews, varied domains, length, and sentiment

#### CNN/DailyMail
- **Source**: https://huggingface.co/datasets/cnn_dailymail
- **Format**: JSON/Parquet
- **License**: Apache 2.0
- **Size**: ~300k articles
- **Pre-2022**: Yes (2007-2015)
- **Characteristics**: News articles, professional journalism, structured prose

### Additional Datasets (Future Expansion)

| Dataset | URL | Notes |
|---------|-----|-------|
| Reddit Corpus | pushshift.io | Informal, varied topics |
| arXiv Papers | arxiv.org/bulk_data | Academic, technical |
| Reuters RCV1 | trec.nist.gov | News, well-curated |
| ICLE v3 | uclouvain.be/en/research-institutes/ilc/cecl/icle.html | Learner essays |
| PubMed | ncbi.nlm.nih.gov/pubmed | Medical abstracts |
| Gigaword | catalog.ldc.upenn.edu | News wire (requires LDC access) |
| IMDb Reviews | ai.stanford.edu/~amaas/data/sentiment/ | Movie reviews |

### Storage Estimates

| Dataset | Raw Size | Processed | Documents |
|---------|----------|-----------|-----------|
| Enron | 1.3 GB | ~200 MB | 25k |
| Yelp | 5 GB | ~300 MB | 50k |
| ASAP | 50 MB | ~40 MB | 25k |
| Stack Overflow | 90 GB | ~500 MB | 50k |
| Amazon | 34 GB | ~300 MB | 50k |
| CNN/DailyMail | 1 GB | ~200 MB | 25k |
| **Total** | ~130 GB | ~1.5 GB | 225k |

### Preprocessing Requirements

```python
# Minimum document requirements
MIN_CHARS = 100
MAX_CHARS = 50000
MIN_WORDS = 20
MAX_WORDS = 10000

# Language detection threshold
ENGLISH_CONFIDENCE = 0.95

# Deduplication
NEAR_DUPLICATE_THRESHOLD = 0.9  # Jaccard similarity

# Quality filters
MIN_WORD_DIVERSITY = 0.3  # Type-token ratio
MAX_BOILERPLATE_RATIO = 0.4  # Repeated phrases
```

### Baseline Metrics to Generate

For each dimension, calculate across the full corpus and per-genre:

```python
BASELINE_STATS = {
    "dimension_name": {
        "mean": float,
        "std": float,
        "min": float,
        "max": float,
        "percentiles": {
            "p5": float,
            "p25": float,
            "p50": float,  # median
            "p75": float,
            "p95": float
        },
        "by_genre": {
            "business": {...},
            "reviews": {...},
            "academic": {...},
            "technical": {...},
            "news": {...},
            "informal": {...}
        }
    }
}
```

### Calibration Workflow

1. **Acquire** datasets using acquisition scripts
2. **Preprocess** to standardized format
3. **Analyze** with all dimensions
4. **Compute** baselines and percentiles
5. **Compare** current thresholds vs empirical distributions
6. **Adjust** thresholds based on findings
7. **Validate** with accuracy tests on known AI/human samples

## Success Metrics

- Corpus contains 200k+ verified human documents across 6+ genres
- Each genre has 10k+ documents for statistical validity
- Baseline generation completes in < 4 hours on standard hardware
- Preprocessing pipeline handles 10k documents/minute
- All datasets have documented provenance and licensing

## Dependencies

- Python 3.9+
- langdetect or fasttext for language detection
- datasketch for deduplication (MinHash LSH)
- tqdm for progress tracking
- requests for downloads
- pyarrow for efficient storage

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-10 | 0.1 | Initial story creation based on deep research | Claude |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes

_To be populated upon completion_

### File List

**New Files:**
- `data/calibration/` - Corpus directory structure
- `data/calibration/manifest.json` - Corpus metadata and tracking
- `data/calibration/README.md` - Documentation
- `scripts/corpus/acquire.py` - Dataset acquisition CLI
- `scripts/corpus/preprocess.py` - Text preprocessing pipeline
- `scripts/corpus/generate_baselines.py` - Baseline metric generation

**Modified Files:**
- `.gitattributes` - Git LFS configuration for large files
- `pyproject.toml` - Add corpus script dependencies (optional)
